{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ffa35c8-7088-487d-b35b-ac321df2a00d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 229 bytes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "log_file_path = \"abfss://bronze@genbiz123.dfs.core.windows.net/logs/processing_log_bronze.log\"\n",
    "logger = logging.getLogger(\"RawDataPipeline\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "class ADLSHandler(logging.Handler):\n",
    "    def emit(self, record):\n",
    "        try:\n",
    "            from pyspark.dbutils import DBUtils\n",
    "            dbutils = DBUtils(spark)\n",
    "            message = self.format(record)\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            full_message = f\"[{timestamp}] {message}\\n\"\n",
    "\n",
    "            try:\n",
    "                existing_logs = dbutils.fs.head(log_file_path, 1048576)\n",
    "            except Exception:\n",
    "                existing_logs = \"\"\n",
    "\n",
    "            updated_logs = existing_logs + full_message\n",
    "            dbutils.fs.put(log_file_path, updated_logs, overwrite=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to write log: {e}\")\n",
    "\n",
    "log_handler = ADLSHandler()\n",
    "formatter = logging.Formatter('%(levelname)s - %(message)s')\n",
    "log_handler.setFormatter(formatter)\n",
    "\n",
    "if not logger.handlers:\n",
    "    logger.addHandler(log_handler)\n",
    "\n",
    "logger.info(\"Logging initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5d25f5f-d1d5-4366-8a7e-8a986f6823ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 289 bytes.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    #importing necessary packages\n",
    "    from pyspark.sql import*\n",
    "    from pyspark.sql.functions import*\n",
    "    import logging\n",
    "    import os\n",
    "    logger.info(\"Library imported successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in Cell 1: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5be483c6-3130-4796-aa87-5ee90c2aa260",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 342 bytes.\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "try:\n",
    "    source_path = \"abfss://raw@blobbizproject.dfs.core.windows.net/\"\n",
    "    destination_base_path = \"abfss://bronze@genbiz123.dfs.core.windows.net/\"\n",
    "    logger.info(\"paths set successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in Cell 4: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30e30ed0-ebf7-49b5-a1af-53bb6c0deca0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 404 bytes.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark.conf.set(\n",
    "    \"fs.azure.account.key.blobbizproject.dfs.core.windows.net\",\n",
    "    \"5E4WuM/5pxbR5Jf7K3GXrD68mYIf1pm4eQvYSX0hunq8iMEpTh7n285i20HBmAyM0U+GGjBmL+Df+ASt4S4EZQ==\")\n",
    "\n",
    "    spark.conf.set(\n",
    "    \"fs.azure.account.key.genbiz123.dfs.core.windows.net\",\n",
    "    \"sv0PmGaLbFlPWbpuUOMhs0EjBV1H+FdY4q92f2xR2CCd4ZwUxYh7gjtlqdLJ4i2JukVV/DX3YzDu+AStouL1lw==\")\n",
    "    logger.info(\"Acess key verified successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in Cell 3: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8749f45-0d86-4cc6-8ad6-a8a0d6f21877",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 460 bytes.\nWrote 521 bytes.\n"
     ]
    }
   ],
   "source": [
    "# Process each CSV file and save as Parquet with audit columns\n",
    "# 1. Delivery Data\n",
    "try:\n",
    "    try:\n",
    "        delivery_df = spark.read.format('csv').option('header','true').option('inferSchema','true').load(f\"{source_path}delivery_data.csv\")\n",
    "        delivery_bronze = delivery_df \\\n",
    "        .withColumn(\"ingestion_date\", current_timestamp()) \\\n",
    "        .withColumn(\"source_file\", lit(\"delivery_data.csv\"))\n",
    "        delivery_bronze.write.mode(\"overwrite\").parquet(f\"{destination_base_path}delivery_data_bronze.parquet\")\n",
    "        logger.info(\"Read operation successful.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in reading data: {e}\")\n",
    "    logger.info(\"Parquet generated successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in Cell 5: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c830a1b1-bbcb-4034-80d8-e6509e0bedbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 577 bytes.\nWrote 638 bytes.\n"
     ]
    }
   ],
   "source": [
    "# 2. Vehicle Data\n",
    "try:\n",
    "    try:  \n",
    "\n",
    "        vehicle_df = spark.read.option(\"header\", \"true\").csv(f\"{source_path}vehicle_data.csv\")\n",
    "        vehicle_bronze = vehicle_df \\\n",
    "        .withColumn(\"ingestion_date\", current_timestamp()) \\\n",
    "        .withColumn(\"source_file\", lit(\"vehicle_data.csv\"))\n",
    "        vehicle_bronze.write.mode(\"overwrite\").parquet(f\"{destination_base_path}vehicle_data_bronze.parquet\")\n",
    "        logger.info(\"Read operation successful.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in reading data: {e}\")\n",
    "    logger.info(\"Parquet generated successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in Cell 8: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c3efafb-eda6-46ce-9785-4f9cccf31fd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 694 bytes.\nWrote 755 bytes.\n"
     ]
    }
   ],
   "source": [
    "# 3. Route Data\n",
    "try:\n",
    "    try:\n",
    "       route_df = spark.read.option(\"header\", \"true\").csv(f\"{source_path}route_data.csv\")\n",
    "       route_bronze = route_df \\\n",
    "       .withColumn(\"ingestion_date\", current_timestamp()) \\\n",
    "       .withColumn(\"source_file\", lit(\"route_data.csv\"))\n",
    "       route_bronze.write.mode(\"overwrite\").parquet(f\"{destination_base_path}route_data_bronze.parquet\")\n",
    "       logger.info(\"Read operation successful.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in reading data: {e}\")\n",
    "    logger.info(\"Parquet generated successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in Cell 7: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51fd32c8-f995-4288-998f-59cc5930a63a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 811 bytes.\nWrote 872 bytes.\n"
     ]
    }
   ],
   "source": [
    "# 4. Driver Data\n",
    "try:\n",
    "    try:\n",
    "        driver_df = spark.read.option(\"header\", \"true\").csv(f\"{source_path}driver_data.csv\")\n",
    "        driver_bronze = driver_df \\\n",
    "        .withColumn(\"ingestion_date\", current_timestamp()) \\\n",
    "        .withColumn(\"source_file\", lit(\"driver_data.csv\"))\n",
    "        driver_bronze.write.mode(\"overwrite\").parquet(f\"{destination_base_path}driver_data_bronze.parquet\")\n",
    "        logger.info(\"Read operation successful.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in reading data: {e}\")\n",
    "    logger.info(\"Parquet generated successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in Cell 6: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ae3a91a-fae6-4511-8e92-9b077518147e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Raw_to_Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}